{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nobr3541/Library/CloudStorage/OneDrive-UCB-O365/Desktop/PhD/Courses/Spring-2025/Robotics_Transformers/Final-project/oasis_ad_prog_dataloader.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['input_features'] = self.scaler.fit_transform(np.vstack(train_df['input_features'])).tolist()\n",
      "/Users/nobr3541/Library/CloudStorage/OneDrive-UCB-O365/Desktop/PhD/Courses/Spring-2025/Robotics_Transformers/Final-project/oasis_ad_prog_dataloader.py:129: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['output_features'] = self.scaler.transform(np.vstack(train_df['output_features'])).tolist()\n",
      "/Users/nobr3541/Library/CloudStorage/OneDrive-UCB-O365/Desktop/PhD/Courses/Spring-2025/Robotics_Transformers/Final-project/oasis_ad_prog_dataloader.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_df['input_features'] = self.scaler.transform(np.vstack(val_df['input_features'])).tolist()\n",
      "/Users/nobr3541/Library/CloudStorage/OneDrive-UCB-O365/Desktop/PhD/Courses/Spring-2025/Robotics_Transformers/Final-project/oasis_ad_prog_dataloader.py:133: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_df['output_features'] = self.scaler.transform(np.vstack(val_df['output_features'])).tolist()\n",
      "/Users/nobr3541/Library/CloudStorage/OneDrive-UCB-O365/Desktop/PhD/Courses/Spring-2025/Robotics_Transformers/Final-project/oasis_ad_prog_dataloader.py:134: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['input_features'] = self.scaler.transform(np.vstack(test_df['input_features'])).tolist()\n",
      "/Users/nobr3541/Library/CloudStorage/OneDrive-UCB-O365/Desktop/PhD/Courses/Spring-2025/Robotics_Transformers/Final-project/oasis_ad_prog_dataloader.py:135: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['output_features'] = self.scaler.transform(np.vstack(test_df['output_features'])).tolist()\n",
      "/Users/nobr3541/Library/CloudStorage/OneDrive-UCB-O365/Desktop/PhD/Courses/Spring-2025/Robotics_Transformers/Final-project/oasis_ad_prog_dataloader.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['input_features'] = self.scaler.fit_transform(np.vstack(train_df['input_features'])).tolist()\n",
      "/Users/nobr3541/Library/CloudStorage/OneDrive-UCB-O365/Desktop/PhD/Courses/Spring-2025/Robotics_Transformers/Final-project/oasis_ad_prog_dataloader.py:129: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['output_features'] = self.scaler.transform(np.vstack(train_df['output_features'])).tolist()\n",
      "/Users/nobr3541/Library/CloudStorage/OneDrive-UCB-O365/Desktop/PhD/Courses/Spring-2025/Robotics_Transformers/Final-project/oasis_ad_prog_dataloader.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_df['input_features'] = self.scaler.transform(np.vstack(val_df['input_features'])).tolist()\n",
      "/Users/nobr3541/Library/CloudStorage/OneDrive-UCB-O365/Desktop/PhD/Courses/Spring-2025/Robotics_Transformers/Final-project/oasis_ad_prog_dataloader.py:133: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_df['output_features'] = self.scaler.transform(np.vstack(val_df['output_features'])).tolist()\n",
      "/Users/nobr3541/Library/CloudStorage/OneDrive-UCB-O365/Desktop/PhD/Courses/Spring-2025/Robotics_Transformers/Final-project/oasis_ad_prog_dataloader.py:134: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['input_features'] = self.scaler.transform(np.vstack(test_df['input_features'])).tolist()\n",
      "/Users/nobr3541/Library/CloudStorage/OneDrive-UCB-O365/Desktop/PhD/Courses/Spring-2025/Robotics_Transformers/Final-project/oasis_ad_prog_dataloader.py:135: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['output_features'] = self.scaler.transform(np.vstack(test_df['output_features'])).tolist()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import nibabel as nib\n",
    "from oasis_ad_prog_dataloader import LongitudinalOASISLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "# Initialize loader\n",
    "loader = LongitudinalOASISLoader(batch_size=batch_size)\n",
    "\n",
    "# Load and preprocess data\n",
    "train_df, val_df, test_df = loader.load_data()\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_ds = loader.create_dataset(train_df, is_train=True)\n",
    "val_ds = loader.create_dataset(val_df)\n",
    "test_ds = loader.create_dataset(test_df)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torchio/data/image.py:248: UserWarning: Using TorchIO images without a torchio.SubjectsLoader in PyTorch >= 2.3 might have unexpected consequences, e.g., the collated batches will be instances of torchio.Subject with 5D images. Replace your PyTorch DataLoader with a torchio.SubjectsLoader so that the collated batch becomes a dictionary, as expected. See https://github.com/fepegar/torchio/issues/1179 for more context about this issue.\n",
      "  warnings.warn(message, stacklevel=1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UCB-O365/Desktop/PhD/Courses/Spring-2025/Robotics_Transformers/Final-project/oasis_ad_prog_dataloader.py:161\u001b[0m, in \u001b[0;36mLongitudinalOASISLoader.create_dataset.<locals>.LongitudinalDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    159\u001b[0m input_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_features\u001b[39m\u001b[38;5;124m'\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m    160\u001b[0m output_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_features\u001b[39m\u001b[38;5;124m'\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m--> 161\u001b[0m demographics \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdemographics\u001b[39m\u001b[38;5;124m'\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m    162\u001b[0m time_delta \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_delta\u001b[39m\u001b[38;5;124m'\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmri\u001b[39m\u001b[38;5;124m'\u001b[39m: input_mri,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m     }\n\u001b[1;32m    175\u001b[0m }\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataLoader named `data_loader`\n",
    "train_images = []\n",
    "train_mmse = []\n",
    "train_input_ids = []\n",
    "train_attention_masks = []\n",
    "train_group = []\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch in train_loader:\n",
    "    train_mmse.append(batch['mmse'])\n",
    "    train_input_ids.append(batch['input_ids'])\n",
    "    train_attention_masks.append(batch['attention_mask'])\n",
    "    train_group.append(batch['group'])\n",
    "\n",
    "\n",
    "# Concatenate all data into single tensors (if needed)\n",
    "\n",
    "train_mmse = torch.cat(train_mmse, dim=0)\n",
    "train_input_ids = torch.cat(train_input_ids, dim=0)\n",
    "train_attention_masks = torch.cat(train_attention_masks, dim=0)\n",
    "\n",
    "# Now you have all the data in tensors\n",
    "print(f\"All MMSE values shape: {train_mmse.shape}\")\n",
    "print(f\"All input IDs shape: {train_input_ids.shape}\")\n",
    "print(f\"All attention masks shape: {train_attention_masks.shape}\")\n",
    "\n",
    "train_mmse_mean = train_mmse.mean().item()\n",
    "train_mmse_std = train_mmse.std().item()\n",
    "train_mmse_min = train_mmse.min().item()\n",
    "train_mmse_max = train_mmse.max().item()\n",
    "train_mmse_median = train_mmse.median().item()\n",
    "\n",
    "print(f\"MMSE Mean: {train_mmse_mean}\")\n",
    "print(f\"MMSE Standard Deviation: {train_mmse_std}\")\n",
    "print(f\"MMSE Min: {train_mmse_min}\")\n",
    "print(f\"MMSE Max: {train_mmse_max}\")\n",
    "print(f\"MMSE Median: {train_mmse_median}\")\n",
    "\n",
    "train_group = torch.cat(train_group, dim=0)\n",
    "# Count the number of each unique value in the train_group array\n",
    "unique_groups, counts = torch.unique(train_group, return_counts=True)\n",
    "\n",
    "# Print the counts for each unique group\n",
    "for group, count in zip(unique_groups, counts):\n",
    "    print(f\"Group {group.item()}: {count.item()} samples\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the MMSE values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(train_mmse.numpy(), bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.title('Distribution of MMSE Values')\n",
    "plt.xlabel('MMSE')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataLoader named `data_loader`\n",
    "val_images = []\n",
    "val_mmse = []\n",
    "val_input_ids = []\n",
    "val_attention_masks = []\n",
    "val_group = []\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch in val_loader:\n",
    "    val_mmse.append(batch['mmse'])\n",
    "    val_input_ids.append(batch['input_ids'])\n",
    "    val_attention_masks.append(batch['attention_mask'])\n",
    "    val_group.append(batch['group'])\n",
    "\n",
    "# Concatenate all data into single tensors (if needed)\n",
    "\n",
    "val_mmse = torch.cat(val_mmse, dim=0)\n",
    "val_input_ids = torch.cat(val_input_ids, dim=0)\n",
    "val_attention_masks = torch.cat(val_attention_masks, dim=0)\n",
    "\n",
    "# Now you have all the data in tensors\n",
    "print(f\"All MMSE values shape: {val_mmse.shape}\")\n",
    "print(f\"All input IDs shape: {val_input_ids.shape}\")\n",
    "print(f\"All attention masks shape: {val_attention_masks.shape}\")\n",
    "# Print descriptive statistics for the MMSE values\n",
    "mmse_mean = val_mmse.mean().item()\n",
    "mmse_std = val_mmse.std().item()\n",
    "mmse_min = val_mmse.min().item()\n",
    "mmse_max = val_mmse.max().item()\n",
    "mmse_median = val_mmse.median().item()\n",
    "\n",
    "print(f\"MMSE Mean: {mmse_mean}\")\n",
    "print(f\"MMSE Standard Deviation: {mmse_std}\")\n",
    "print(f\"MMSE Min: {mmse_min}\")\n",
    "print(f\"MMSE Max: {mmse_max}\")\n",
    "print(f\"MMSE Median: {mmse_median}\")\n",
    "\n",
    "val_group = torch.cat(val_group, dim=0)\n",
    "# Count the number of each unique value in the val_group array\n",
    "unique_groups, counts = torch.unique(val_group, return_counts=True)\n",
    "\n",
    "# Print the counts for each unique group\n",
    "for group, count in zip(unique_groups, counts):\n",
    "    print(f\"Group {group.item()}: {count.item()} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the MMSE values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(val_mmse.numpy(), bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.title('Distribution of MMSE Values')\n",
    "plt.xlabel('MMSE')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataLoader named `data_loader`\n",
    "test_images = []\n",
    "test_mmse = []\n",
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "test_group = []\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch in test_loader:\n",
    "    test_mmse.append(batch['mmse'])\n",
    "    test_input_ids.append(batch['input_ids'])\n",
    "    test_attention_masks.append(batch['attention_mask'])\n",
    "    test_group.append(batch['group'])\n",
    "\n",
    "# Concatenate all data into single tensors (if needed)\n",
    "\n",
    "test_mmse = torch.cat(test_mmse, dim=0)\n",
    "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
    "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
    "\n",
    "# Now you have all the data in tensors\n",
    "print(f\"All MMSE values shape: {test_mmse.shape}\")\n",
    "print(f\"All input IDs shape: {test_input_ids.shape}\")\n",
    "print(f\"All attention masks shape: {test_attention_masks.shape}\")\n",
    "\n",
    "test_group = torch.cat(test_group, dim=0)\n",
    "unique_groups, counts = torch.unique(test_group, return_counts=True)\n",
    "\n",
    "# Print the counts for each unique group\n",
    "for group, count in zip(unique_groups, counts):\n",
    "    print(f\"Group {group.item()}: {count.item()} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the MMSE values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(test_mmse.numpy(), bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.title('Distribution of MMSE Values')\n",
    "plt.xlabel('MMSE')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial\n",
    "\n",
    "k = 195  # Total countries\n",
    "n = 30   # Students\n",
    "\n",
    "prob_all_different = factorial(k) / (factorial(k - n) * (k ** n))\n",
    "print(prob_all_different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
