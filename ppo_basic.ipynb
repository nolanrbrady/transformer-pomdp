{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f78e36ac-a491-46cf-bb00-b25b5c49f1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy, ActorCriticCnnPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.ppo import PPO\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from gymnasium.wrappers import ResizeObservation\n",
    "from vizdoom import gymnasium_wrapper\n",
    "\n",
    "# Import model\n",
    "from models.basic_vit import BasicViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73821c59-a8a4-4373-b961-dcac396fd57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch device check: cpu\n",
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Observation space: Dict('gamevariables': Box(-3.4028235e+38, 3.4028235e+38, (1,), float32), 'screen': Box(0, 255, (3, 240, 320), uint8))\n",
      "Shape from observation space: (3, 240, 320)\n",
      "Extracted dimensions: h=84, w=84, c=3\n",
      "Will create 36 patches (6x6) with patch_size=14\n",
      "BasicViT model initialized on: cpu\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 203      |\n",
      "|    ep_rew_mean     | -210     |\n",
      "| time/              |          |\n",
      "|    fps             | 93       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 87       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 201          |\n",
      "|    ep_rew_mean          | -204         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 21           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 758          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077371933 |\n",
      "|    clip_fraction        | 0.0998       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.000129     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 150          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00218     |\n",
      "|    value_loss           | 224          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 189          |\n",
      "|    ep_rew_mean          | -179         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 17           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 1417         |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046739457 |\n",
      "|    clip_fraction        | 0.00355      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 174          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | 0.000281     |\n",
      "|    value_loss           | 338          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 186         |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2081        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012648823 |\n",
      "|    clip_fraction        | 0.072       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 149         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00166    |\n",
      "|    value_loss           | 507         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 160         |\n",
      "|    ep_rew_mean          | -141        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 2737        |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006439037 |\n",
      "|    clip_fraction        | 0.0576      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 306         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.000715   |\n",
      "|    value_loss           | 445         |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Custom Features Extractor using BasicViT\n",
    "class ViTFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=512):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        \n",
    "        # Print raw observation space for debugging\n",
    "        print(f\"Observation space: {observation_space}\")\n",
    "        \n",
    "        screen_space = observation_space['screen']\n",
    "        shape = screen_space.shape\n",
    "            \n",
    "        print(f\"Shape from observation space: {shape}\")\n",
    "        \n",
    "        # VecTransposeImage wrapper changes format to channels-first (C, H, W)\n",
    "        if len(shape) == 3 and shape[0] == 3:\n",
    "            # Channels first format (C, H, W)\n",
    "            c, h, w = shape\n",
    "        else:\n",
    "            # Assume channels last format (H, W, C)\n",
    "            h, w, c = shape\n",
    "\n",
    "        h = 84\n",
    "        w = 84\n",
    "        c = 3\n",
    "            \n",
    "        print(f\"Dimensions used: h={h}, w={w}, c={c}\")\n",
    "        \n",
    "        # Calculate patches\n",
    "        patch_size = 14\n",
    "        n_h = h // patch_size\n",
    "        n_w = w // patch_size\n",
    "        n_patches = n_h * n_w\n",
    "        \n",
    "        print(f\"Will create {n_patches} patches ({n_h}x{n_w}) with patch_size={patch_size}\")\n",
    "\n",
    "        # Set device\n",
    "        device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Create the ViT with fixed in_channels=3 for RGB\n",
    "        self.vit = BasicViT(\n",
    "            img_size=(h, w),\n",
    "            patch_size=patch_size,\n",
    "            in_channels=3,  # IMPORTANT: Force to 3 for RGB images\n",
    "            num_classes=features_dim,\n",
    "            embed_dim=features_dim,\n",
    "            num_blocks=6,\n",
    "            num_heads=4,\n",
    "            mlp_ratio=2.0,\n",
    "            dropout=0.1,\n",
    "            pad_if_needed=True,\n",
    "            device=device,  # Explicitly pass the device\n",
    "        )\n",
    "\n",
    "    def forward(self, observations):\n",
    "        # Handle dict observations\n",
    "        if isinstance(observations, dict):\n",
    "            obs = observations['screen']\n",
    "        else:\n",
    "            obs = observations\n",
    "        \n",
    "        # Ensure BCHW format for PyTorch\n",
    "        obs = obs.float()\n",
    "        if len(obs.shape) == 4:\n",
    "            # If batch dimension exists\n",
    "            if obs.shape[1] == 3:\n",
    "                # Already in BCHW format\n",
    "                pass\n",
    "            elif obs.shape[3] == 3:\n",
    "                # Convert BHWC to BCHW\n",
    "                obs = obs.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Debug the model outputs\n",
    "        features = self.vit(obs)\n",
    "        # print(f\"[ViT] Feature vector mean: {features.mean().item():.4f}, std: {features.std().item():.4f}\")\n",
    "        # print(f\"[ViT] Feature sample: {features[0][:5]}\")\n",
    "        feature_var_across_batch = features.var(dim=0).mean().item()\n",
    "        # print(f\"[ViT] Avg feature variance across batch: {feature_var_across_batch:.6f}\")\n",
    "        return features\n",
    "\n",
    "# # Custom Policy\n",
    "# class CustomViTPolicy(ActorCriticPolicy):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(\n",
    "#             *args,\n",
    "#             **kwargs,\n",
    "#             features_extractor_class=ViTFeatureExtractor,\n",
    "#             features_extractor_kwargs=dict(features_dim=512),\n",
    "#         )\n",
    "\n",
    "print(f\"PyTorch device check: {th.device('cuda' if th.cuda.is_available() else 'cpu')}\")\n",
    "\n",
    "# Environment Setup\n",
    "env = make_vec_env(\"VizdoomBasic-v0\", n_envs=4)\n",
    "obs_space = env.observation_space['screen']\n",
    "act_space = env.action_space.n\n",
    "img_height, img_width, channels = obs_space.shape\n",
    "\n",
    "# Train PPO Agent\n",
    "model = PPO(\n",
    "    \"MultiInputPolicy\", \n",
    "    env,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=ViTFeatureExtractor,\n",
    "        features_extractor_kwargs=dict(features_dim=512)\n",
    "    ),\n",
    "    verbose=1\n",
    ")\n",
    "model.learn(total_timesteps=100_000)\n",
    "model.save(\"ppo_basic_vit_vizdoom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffbeea5-6310-4d4e-9d35-353d84c1cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 210      |\n",
      "|    ep_rew_mean     | -213     |\n",
      "| time/              |          |\n",
      "|    fps             | 92       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 88       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 202         |\n",
      "|    ep_rew_mean          | -198        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 684         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005508961 |\n",
      "|    clip_fraction        | 0.0514      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | -0.00129    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.8        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00199    |\n",
      "|    value_loss           | 168         |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.torch_layers import NatureCNN\n",
    "from stable_baselines3.common.policies import MultiInputActorCriticPolicy\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "import gym\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "class ScreenOnlyCNN(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.spaces.Dict, features_dim: int = 512):\n",
    "        # Extract the screen space\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        # This assumes the screen is (C, H, W)\n",
    "        self.cnn = NatureCNN(observation_space.spaces[\"screen\"], features_dim)\n",
    "\n",
    "    def forward(self, observations):\n",
    "        return self.cnn(observations[\"screen\"])\n",
    "\n",
    "# Setup environment\n",
    "env = make_vec_env(\"VizdoomBasic-v0\", n_envs=4)\n",
    "\n",
    "# Set policy_kwargs to use custom feature extractor\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=ScreenOnlyCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=512)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model = PPO(\"MultiInputPolicy\", env, policy_kwargs=policy_kwargs, verbose=1)\n",
    "model.learn(total_timesteps=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cee033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
