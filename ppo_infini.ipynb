{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f78e36ac-a491-46cf-bb00-b25b5c49f1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.ppo import PPO\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from collections import deque\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from gymnasium.wrappers import ResizeObservation\n",
    "from vizdoom import gymnasium_wrapper\n",
    "\n",
    "# Import model\n",
    "from models.infini_vit import InfiniViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73821c59-a8a4-4373-b961-dcac396fd57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch device check: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Observation space: Dict('gamevariables': Box(-3.4028235e+38, 3.4028235e+38, (1,), float32), 'screen': Box(0, 255, (3, 240, 320), uint8))\n",
      "Shape from observation space: (3, 240, 320)\n",
      "Extracted dimensions: h=240, w=320, c=3\n",
      "Will create 1200 patches (30x40) with patch_size=8\n",
      "Using grid of 30x40 patches = 1200 total patches\n",
      "TemporalViT model initialized on: cpu\n",
      "Initialized frame buffers for 4 environments with 4 frames each\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 190      |\n",
      "|    ep_rew_mean     | -189     |\n",
      "| time/              |          |\n",
      "|    fps             | 9        |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 874      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Initialized frame buffers for 64 environments with 4 frames each\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Custom Features Extractor using BasicViT\n",
    "class ViTFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=512, frame_history=4):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        \n",
    "        # Frame history length\n",
    "        self.frame_history = frame_history\n",
    "        self.observation_space = observation_space\n",
    "        \n",
    "        # Print raw observation space for debugging\n",
    "        print(f\"Observation space: {self.observation_space}\")\n",
    "        \n",
    "        screen_space = self.observation_space['screen']\n",
    "        shape = screen_space.shape\n",
    "            \n",
    "        print(f\"Shape from observation space: {shape}\")\n",
    "        \n",
    "        # VecTransposeImage wrapper changes format to channels-first (C, H, W)\n",
    "        if len(shape) == 3 and shape[0] == 3:\n",
    "            # Channels first format (C, H, W)\n",
    "            c, h, w = shape\n",
    "        else:\n",
    "            # Assume channels last format (H, W, C)\n",
    "            h, w, c = shape\n",
    "            \n",
    "        print(f\"Extracted dimensions: h={h}, w={w}, c={c}\")\n",
    "        \n",
    "        # Calculate patches\n",
    "        patch_size = 8\n",
    "        n_h = h // patch_size\n",
    "        n_w = w // patch_size\n",
    "        n_patches = n_h * n_w\n",
    "        \n",
    "        print(f\"Will create {n_patches} patches ({n_h}x{n_w}) with patch_size={patch_size}\")\n",
    "\n",
    "        # Set device\n",
    "        device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Create the ViT with fixed in_channels=3 for RGB\n",
    "        self.vit = InfiniViT(\n",
    "            img_size=(h, w),\n",
    "            patch_size=patch_size,\n",
    "            in_channels=3,  # IMPORTANT: Force to 3 for RGB images\n",
    "            num_classes=features_dim,\n",
    "            embed_dim=features_dim,\n",
    "            num_heads=8,\n",
    "            mlp_ratio=2.0,\n",
    "            memory_size=128,\n",
    "            window_size=1,\n",
    "            dropout=0.1,\n",
    "            pad_if_needed=True,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Initialize frame history buffers \n",
    "        self.frame_buffers = None\n",
    "        self.initialized = False\n",
    "        \n",
    "    def reset_history(self, batch_size):\n",
    "        \"\"\"Initialize or reset frame history for all environments\"\"\"\n",
    "        # Create a default empty tensor for initial frames\n",
    "        empty_frame = th.zeros((3, *self.observation_space['screen'].shape[1:]), \n",
    "                               dtype=th.float32,\n",
    "                               device=self.vit.device)\n",
    "        \n",
    "        # Create a frame buffer for each environment in the batch\n",
    "        self.frame_buffers = []\n",
    "        for _ in range(batch_size):\n",
    "            buffer = []\n",
    "            for _ in range(self.frame_history):\n",
    "                # Create a new tensor for each frame to avoid reference issues\n",
    "                buffer.append(empty_frame.clone())\n",
    "            self.frame_buffers.append(buffer)\n",
    "        \n",
    "        self.initialized = True\n",
    "        print(f\"Initialized frame buffers for {batch_size} environments with {self.frame_history} frames each\")\n",
    "\n",
    "    def update_frames(self, observations):\n",
    "        \"\"\"Update frame history with new observations\"\"\"\n",
    "        if isinstance(observations, dict):\n",
    "            obs = observations['screen']\n",
    "        else:\n",
    "            obs = observations\n",
    "            \n",
    "        # Initialize buffers if needed\n",
    "        batch_size = obs.shape[0]\n",
    "        if not self.initialized or self.frame_buffers is None or len(self.frame_buffers) != batch_size:\n",
    "            self.reset_history(batch_size)\n",
    "        \n",
    "        # Update each environment's frame buffer by replacing the oldest frame\n",
    "        for i in range(batch_size):\n",
    "            # Remove oldest frame and add new frame\n",
    "            self.frame_buffers[i].pop(0)\n",
    "            self.frame_buffers[i].append(obs[i].clone().to(self.vit.device))\n",
    "            \n",
    "        return obs\n",
    "\n",
    "    def forward(self, observations):\n",
    "        \"\"\"Process observations through the temporal ViT model\"\"\"\n",
    "        # Update frame history with new observations\n",
    "        obs = self.update_frames(observations)\n",
    "        \n",
    "        # Create a batch of temporal sequences\n",
    "        batch_size = obs.shape[0]\n",
    "        features = []\n",
    "        \n",
    "        # Process each environment's frame history separately\n",
    "        for i in range(batch_size):\n",
    "            # Stack frames along first dimension to create temporal sequence (T, C, H, W)\n",
    "            frame_sequence = th.stack(self.frame_buffers[i], dim=0)\n",
    "            \n",
    "            # Pass the sequence through TemporalViT\n",
    "            feature = self.vit(frame_sequence)\n",
    "            features.append(feature)\n",
    "            \n",
    "        # Stack all features into a batch\n",
    "        return th.stack(features, dim=0)\n",
    "\n",
    "# Custom Policy\n",
    "class CustomViTPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            **kwargs,\n",
    "            features_extractor_class=ViTFeatureExtractor,\n",
    "            features_extractor_kwargs=dict(features_dim=512),\n",
    "        )\n",
    "\n",
    "print(f\"PyTorch device check: {th.device('cuda' if th.cuda.is_available() else 'cpu')}\")\n",
    "\n",
    "# Environment Setup\n",
    "env = make_vec_env(\"VizdoomBasic-v0\", n_envs=4)\n",
    "obs_space = env.observation_space['screen']\n",
    "act_space = env.action_space.n\n",
    "img_height, img_width, channels = obs_space.shape\n",
    "\n",
    "# Train PPO Agent\n",
    "model = PPO(CustomViTPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=100_000)\n",
    "model.save(\"ppo_vit_vizdoom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffbeea5-6310-4d4e-9d35-353d84c1cc1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
